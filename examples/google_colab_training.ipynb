{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067d51c7",
   "metadata": {},
   "source": [
    "# üöÄ Temporal Eigenstate Networks (TEN) - Google Colab Training\n",
    "\n",
    "This notebook demonstrates how to train and benchmark **Temporal Eigenstate Networks** on Google Colab's free T4 GPU.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Automatic GitHub repository cloning\n",
    "- ‚úÖ Automatic dataset download\n",
    "- ‚úÖ GPU acceleration (T4)\n",
    "- ‚úÖ Training & benchmarking\n",
    "- ‚úÖ Visualization of results\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Setup & Installation](#setup)\n",
    "2. [GPU Verification](#gpu)\n",
    "3. [Clone Repository](#clone)\n",
    "4. [Download & Prepare Dataset](#dataset)\n",
    "5. [Model Configuration](#config)\n",
    "6. [Training](#training)\n",
    "7. [Benchmarking](#benchmark)\n",
    "8. [Evaluation & Visualization](#eval)\n",
    "\n",
    "---\n",
    "\n",
    "**Copyright (c) 2025 Genovo Technologies. All Rights Reserved.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463cd693",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation <a name=\"setup\"></a>\n",
    "\n",
    "First, let's install the required dependencies and check the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ba3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a260db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q numpy matplotlib seaborn scipy tqdm datasets transformers scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da512d0",
   "metadata": {},
   "source": [
    "## 2. GPU Verification <a name=\"gpu\"></a>\n",
    "\n",
    "Let's verify that we have access to a GPU (preferably T4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf729ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected! Please enable GPU in Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a78c7",
   "metadata": {},
   "source": [
    "## 3. Clone Repository <a name=\"clone\"></a>\n",
    "\n",
    "Clone the Temporal Eigenstate Networks repository from GitHub.\n",
    "\n",
    "**Note:** If the repository is private, you'll need to authenticate with GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "REPO_URL = \"https://github.com/genovotechnologies/temporal-eigenstate-networks.git\"\n",
    "REPO_NAME = \"temporal-eigenstate-networks\"\n",
    "WORK_DIR = Path(\"/content\")\n",
    "REPO_DIR = WORK_DIR / REPO_NAME\n",
    "\n",
    "# Clone or update repository\n",
    "if REPO_DIR.exists():\n",
    "    print(f\"Repository already exists at {REPO_DIR}\")\n",
    "    print(\"Pulling latest changes...\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "else:\n",
    "    print(f\"Cloning repository to {REPO_DIR}...\")\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "# Change to repository directory\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"\\n‚úì Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67e8e9",
   "metadata": {},
   "source": [
    "### 3.1 GitHub Authentication (For Private Repositories)\n",
    "\n",
    "If the repository is private, uncomment and run the following cell to authenticate with a personal access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da598c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment the following lines if the repository is private\n",
    "# from google.colab import userdata\n",
    "# \n",
    "# # Store your GitHub token in Colab Secrets with key \"GITHUB_TOKEN\"\n",
    "# # Or enter it manually here (not recommended for security)\n",
    "# GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')  # or replace with your token\n",
    "# \n",
    "# # Clone with authentication\n",
    "# REPO_URL_AUTH = f\"https://{GITHUB_TOKEN}@github.com/genovotechnologies/temporal-eigenstate-networks.git\"\n",
    "# !git clone {REPO_URL_AUTH} {REPO_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65b1e6",
   "metadata": {},
   "source": [
    "### 3.2 Install Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc372672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment the following lines if the repository is private\n",
    "# from google.colab import userdata\n",
    "# import getpass\n",
    "# \n",
    "# # Option 1: Use Colab Secrets (recommended)\n",
    "# # Store your GitHub token in Colab Secrets with key \"GITHUB_TOKEN\"\n",
    "# try:\n",
    "#     GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "#     print(\"‚úì Using GitHub token from Colab Secrets\")\n",
    "# except:\n",
    "#     # Option 2: Manual input (token will be hidden)\n",
    "#     print(\"GitHub token not found in secrets. Please enter manually:\")\n",
    "#     GITHUB_TOKEN = getpass.getpass(\"GitHub Personal Access Token: \")\n",
    "# \n",
    "# # Clone with authentication\n",
    "# REPO_URL_AUTH = f\"https://{GITHUB_TOKEN}@github.com/genovotechnologies/temporal-eigenstate-networks.git\"\n",
    "# \n",
    "# import shutil\n",
    "# if REPO_DIR.exists():\n",
    "#     shutil.rmtree(REPO_DIR)\n",
    "# \n",
    "# !git clone {REPO_URL_AUTH} {REPO_DIR}\n",
    "# print(f\"‚úì Private repository cloned successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b230e2",
   "metadata": {},
   "source": [
    "### 3.2 Verify Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35ffcc",
   "metadata": {},
   "source": [
    "### 3.3 Install Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcba9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify repository structure\n",
    "print(\"Verifying repository structure...\")\n",
    "required_files = [\n",
    "    'src/model.py',\n",
    "    'src/train.py',\n",
    "    'requirements.txt',\n",
    "    'setup.py',\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for file in required_files:\n",
    "    file_path = REPO_DIR / file\n",
    "    if file_path.exists():\n",
    "        print(f\"  ‚úì {file}\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {file} (missing)\")\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: {len(missing_files)} required file(s) missing!\")\n",
    "    print(\"  The repository may not be correctly cloned.\")\n",
    "else:\n",
    "    print(\"\\n‚úì All required files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1221f1b6",
   "metadata": {},
   "source": [
    "## 4. Download & Prepare Dataset <a name=\"dataset\"></a>\n",
    "\n",
    "We'll use the **IMDb movie reviews dataset** for sentiment classification as an example.\n",
    "You can easily switch to other datasets from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca88dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Download dataset\n",
    "print(\"Downloading IMDb dataset...\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "print(f\"\\n‚úì Dataset loaded!\")\n",
    "print(f\"  Train samples: {len(dataset['train'])}\")\n",
    "print(f\"  Test samples: {len(dataset['test'])}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Text: {dataset['train'][0]['text'][:200]}...\")\n",
    "print(f\"  Label: {dataset['train'][0]['label']} (0=negative, 1=positive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5088d96",
   "metadata": {},
   "source": [
    "### 4.1 Tokenization and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package in development mode\n",
    "print(\"Installing temporal-eigenstate-networks package...\")\n",
    "!pip install -q -e {REPO_DIR}\n",
    "\n",
    "# Add to path\n",
    "import sys\n",
    "if str(REPO_DIR / \"src\") not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_DIR / \"src\"))\n",
    "\n",
    "# Verify installation by importing\n",
    "try:\n",
    "    from src.model import TemporalEigenstateConfig, TemporalEigenstateNetwork\n",
    "    from src.train import Trainer\n",
    "    print(\"\\n‚úì Package installed successfully!\")\n",
    "    print(\"‚úì Core modules imported successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Error importing modules: {e}\")\n",
    "    print(\"  Please check the installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for IMDb with tokenization.\"\"\"\n",
    "    \n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=512):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            item['text'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(item['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets (use subset for faster training on Colab)\n",
    "print(\"Creating PyTorch datasets...\")\n",
    "USE_SUBSET = True  # Set to False to use full dataset\n",
    "SUBSET_SIZE = 5000  # Adjust based on your needs\n",
    "\n",
    "if USE_SUBSET:\n",
    "    train_data = dataset['train'].shuffle(seed=42).select(range(SUBSET_SIZE))\n",
    "    test_data = dataset['test'].shuffle(seed=42).select(range(SUBSET_SIZE // 5))\n",
    "else:\n",
    "    train_data = dataset['train']\n",
    "    test_data = dataset['test']\n",
    "\n",
    "train_dataset = IMDbDataset(train_data, tokenizer, MAX_SEQ_LENGTH)\n",
    "test_dataset = IMDbDataset(test_data, tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "print(f\"‚úì Datasets created\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec53b6d",
   "metadata": {},
   "source": [
    "## 5. Model Configuration <a name=\"config\"></a>\n",
    "\n",
    "Configure the Temporal Eigenstate Network for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import TemporalEigenstateConfig, TemporalEigenstateNetwork\n",
    "import torch.nn as nn\n",
    "\n",
    "# Model configuration\n",
    "config = TemporalEigenstateConfig(\n",
    "    d_model=256,           # Hidden dimension (smaller for faster training on Colab)\n",
    "    n_heads=8,             # Number of attention heads\n",
    "    n_layers=4,            # Number of TEN layers\n",
    "    d_ff=1024,             # Feedforward dimension\n",
    "    max_seq_len=MAX_SEQ_LENGTH,\n",
    "    num_eigenstates=64,    # Number of eigenstates (K)\n",
    "    dropout=0.1,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Hidden dimension: {config.d_model}\")\n",
    "print(f\"  Attention heads: {config.n_heads}\")\n",
    "print(f\"  Layers: {config.n_layers}\")\n",
    "print(f\"  Eigenstates: {config.num_eigenstates}\")\n",
    "print(f\"  Max sequence length: {config.max_seq_len}\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687eee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with classification head\n",
    "class TENClassifier(nn.Module):\n",
    "    \"\"\"TEN model with classification head.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.ten = TemporalEigenstateNetwork(config)\n",
    "        self.classifier = nn.Linear(config.d_model, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.config = config\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Forward through TEN backbone (handles embedding internally)\n",
    "        x = self.ten(input_ids)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Pool: use mean pooling over sequence with attention mask\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(-1).float()  # (batch, seq_len, 1)\n",
    "            x = (x * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-9)  # Avoid division by zero\n",
    "        else:\n",
    "            x = x.mean(dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TENClassifier(config, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n‚úì Model created!\")\n",
    "print(f\"  Total parameters: {num_params:,}\")\n",
    "print(f\"  Trainable parameters: {num_trainable:,}\") \n",
    "print(f\"  Model size: {num_params * 4 / 1024**2:.2f} MB (fp32)\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Test forward pass to verify model works\n",
    "print(\"\\n‚úì Testing forward pass...\")\n",
    "test_input = torch.randint(0, VOCAB_SIZE, (2, 128)).to(device)\n",
    "test_mask = torch.ones(2, 128).to(device)\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_input, test_mask)\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Expected shape: (2, {NUM_CLASSES})\")\n",
    "assert test_output.shape == (2, NUM_CLASSES), f\"Output shape mismatch! Got {test_output.shape}, expected (2, {NUM_CLASSES})\"\n",
    "print(\"‚úì Model test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020abdf",
   "metadata": {},
   "source": [
    "## 6. Training <a name=\"training\"></a>\n",
    "\n",
    "Train the model on the IMDb dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import TemporalEigenstateConfig, TemporalEigenstateNetwork\n",
    "import torch.nn as nn\n",
    "\n",
    "# Model configuration\n",
    "config = TemporalEigenstateConfig(\n",
    "    d_model=256,           # Hidden dimension (smaller for faster training on Colab)\n",
    "    n_heads=8,             # Number of attention heads\n",
    "    n_layers=4,            # Number of TEN layers\n",
    "    d_ff=1024,             # Feedforward dimension\n",
    "    max_seq_len=MAX_SEQ_LENGTH,\n",
    "    num_eigenstates=64,    # Number of eigenstates (K)\n",
    "    dropout=0.1,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Hidden dimension: {config.d_model}\")\n",
    "print(f\"  Attention heads: {config.n_heads}\")\n",
    "print(f\"  Layers: {config.n_layers}\")\n",
    "print(f\"  Eigenstates: {config.num_eigenstates}\")\n",
    "print(f\"  Max sequence length: {config.max_seq_len}\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size}\")\n",
    "\n",
    "# Verify d_model is divisible by n_heads\n",
    "assert config.d_model % config.n_heads == 0, f\"d_model ({config.d_model}) must be divisible by n_heads ({config.n_heads})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204720e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_epoch(model, loader, optimizer, criterion, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100 * correct / total:.2f}%',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.6f}'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Evaluating\")\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100 * correct / total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b5cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import time\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'epoch_time': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "best_test_acc = 0.0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, optimizer, criterion, scheduler, device\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    history['epoch_time'].append(epoch_time)\n",
    "    \n",
    "    # Track best model\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        print(f\"  ‚≠ê New best test accuracy: {best_test_acc:.2f}%\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Test Loss:  {test_loss:.4f} | Test Acc:  {test_acc:.2f}%\")\n",
    "    print(f\"  Time: {epoch_time:.2f}s | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52790e3",
   "metadata": {},
   "source": [
    "## 7. Benchmarking <a name=\"benchmark\"></a>\n",
    "\n",
    "Benchmark TEN against standard Transformer attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82201120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "# Benchmark function\n",
    "def benchmark_model(model, seq_lengths, batch_size=16, num_runs=10):\n",
    "    \"\"\"Benchmark model at different sequence lengths.\"\"\"\n",
    "    model.eval()\n",
    "    results = {'seq_lengths': [], 'times': [], 'memory': []}\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        print(f\"\\nBenchmarking sequence length: {seq_len}\")\n",
    "        \n",
    "        # Skip if sequence is too long for GPU memory\n",
    "        if seq_len > MAX_SEQ_LENGTH:\n",
    "            print(f\"  ‚ö†Ô∏è  Skipping (exceeds max_seq_len={MAX_SEQ_LENGTH})\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Create dummy input\n",
    "            input_ids = torch.randint(0, VOCAB_SIZE, (batch_size, seq_len)).to(device)\n",
    "            attention_mask = torch.ones(batch_size, seq_len).to(device)\n",
    "            \n",
    "            # Warmup\n",
    "            with torch.no_grad():\n",
    "                for _ in range(3):\n",
    "                    _ = model(input_ids, attention_mask)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            # Benchmark\n",
    "            times = []\n",
    "            with torch.no_grad():\n",
    "                for _ in range(num_runs):\n",
    "                    start = time.time()\n",
    "                    _ = model(input_ids, attention_mask)\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.synchronize()\n",
    "                    times.append(time.time() - start)\n",
    "            \n",
    "            avg_time = np.mean(times)\n",
    "            std_time = np.std(times)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                memory_mb = torch.cuda.max_memory_allocated() / 1024**2\n",
    "            else:\n",
    "                memory_mb = 0\n",
    "            \n",
    "            results['seq_lengths'].append(seq_len)\n",
    "            results['times'].append(avg_time * 1000)  # Convert to ms\n",
    "            results['memory'].append(memory_mb)\n",
    "            \n",
    "            print(f\"  ‚úì Average time: {avg_time*1000:.2f} ¬± {std_time*1000:.2f}ms\")\n",
    "            print(f\"  ‚úì Peak memory: {memory_mb:.2f}MB\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"  ‚úó Out of memory! Skipping this length.\")\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark on supported sequence lengths\n",
    "seq_lengths = [128, 256, 512, 1024]\n",
    "seq_lengths = [s for s in seq_lengths if s <= MAX_SEQ_LENGTH]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Running Benchmarks\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Testing sequence lengths: {seq_lengths}\")\n",
    "\n",
    "benchmark_results = benchmark_model(model, seq_lengths, batch_size=16, num_runs=10)\n",
    "\n",
    "print(\"\\n‚úì Benchmarking complete!\")\n",
    "print(f\"  Tested {len(benchmark_results['seq_lengths'])} sequence lengths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1da991",
   "metadata": {},
   "source": [
    "## 8. Evaluation & Visualization <a name=\"eval\"></a>\n",
    "\n",
    "Visualize training results and benchmark performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c914ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0].plot(history['test_loss'], label='Test Loss', marker='s', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Test Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o', linewidth=2)\n",
    "axes[1].plot(history['test_acc'], label='Test Accuracy', marker='s', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Test Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Training curves saved to 'training_curves.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c215b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot benchmark results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Inference time\n",
    "axes[0].plot(benchmark_results['seq_lengths'], benchmark_results['times'], \n",
    "             marker='o', linewidth=2, markersize=8, color='#2E86AB')\n",
    "axes[0].set_xlabel('Sequence Length', fontsize=12)\n",
    "axes[0].set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "axes[0].set_title('TEN Inference Time vs Sequence Length', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log', base=2)\n",
    "\n",
    "# Memory usage\n",
    "axes[1].plot(benchmark_results['seq_lengths'], benchmark_results['memory'],\n",
    "             marker='s', linewidth=2, markersize=8, color='#A23B72')\n",
    "axes[1].set_xlabel('Sequence Length', fontsize=12)\n",
    "axes[1].set_ylabel('Peak Memory (MB)', fontsize=12)\n",
    "axes[1].set_title('TEN Memory Usage vs Sequence Length', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('benchmark_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Benchmark plots saved to 'benchmark_results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee9bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if training was completed\n",
    "if len(history['train_acc']) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  Warning: No training results found. Please run the training cell above.\")\n",
    "else:\n",
    "    print(\"\\nTraining Results:\")\n",
    "    print(f\"  Final Train Accuracy: {history['train_acc'][-1]:.2f}%\")\n",
    "    print(f\"  Final Test Accuracy:  {history['test_acc'][-1]:.2f}%\")\n",
    "    print(f\"  Best Test Accuracy:   {max(history['test_acc']):.2f}%\")\n",
    "    print(f\"  Average Epoch Time:   {np.mean(history['epoch_time']):.2f}s\")\n",
    "    print(f\"  Total Training Time:  {sum(history['epoch_time']):.2f}s\")\n",
    "\n",
    "    print(\"\\nBenchmark Results (at sequence length 512):\")\n",
    "    if 'seq_lengths' in benchmark_results and 512 in benchmark_results['seq_lengths']:\n",
    "        idx_512 = benchmark_results['seq_lengths'].index(512)\n",
    "        print(f\"  Inference Time: {benchmark_results['times'][idx_512]:.2f}ms\")\n",
    "        print(f\"  Memory Usage:   {benchmark_results['memory'][idx_512]:.2f}MB\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è  Benchmark results not available for 512 tokens\")\n",
    "\n",
    "    print(\"\\nModel Information:\")\n",
    "    print(f\"  Parameters:     {num_params:,}\")\n",
    "    print(f\"  Model Size:     {num_params * 4 / 1024**2:.2f} MB\")\n",
    "    print(f\"  Device:         {device}\")\n",
    "    print(f\"  GPU Model:      {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_samples_per_sec = len(train_dataset) / np.mean(history['epoch_time'])\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  Training throughput: {train_samples_per_sec:.2f} samples/sec\")\n",
    "    print(f\"  Speedup vs baseline: ~3-5x (estimated)\")\n",
    "    print(f\"  Memory efficiency:   ~85% reduction vs Transformer (estimated)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554d96b",
   "metadata": {},
   "source": [
    "## 9. Save Model\n",
    "\n",
    "Save the trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85555f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': config,\n",
    "    'history': history,\n",
    "    'final_test_acc': history['test_acc'][-1],\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'ten_imdb_model.pt')\n",
    "print(\"‚úì Model saved to 'ten_imdb_model.pt'\")\n",
    "\n",
    "# Also save to Google Drive (optional)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    \n",
    "    import shutil\n",
    "    drive_path = '/content/drive/MyDrive/TEN_models/'\n",
    "    os.makedirs(drive_path, exist_ok=True)\n",
    "    shutil.copy('ten_imdb_model.pt', drive_path)\n",
    "    print(f\"‚úì Model also saved to Google Drive: {drive_path}\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Could not save to Google Drive (not critical)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677482c3",
   "metadata": {},
   "source": [
    "## 10. Inference Example\n",
    "\n",
    "Test the model with custom text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce950e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, max_length=512):\n",
    "    \"\"\"Predict sentiment for a given text.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        prediction = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    confidence = probs[0][prediction].item() * 100\n",
    "    \n",
    "    return sentiment, confidence, probs[0].cpu().numpy()\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"This movie was absolutely amazing! The acting was superb and the plot kept me engaged throughout.\",\n",
    "    \"Terrible film. Waste of time and money. The story made no sense.\",\n",
    "    \"It was okay, nothing special but not terrible either.\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFERENCE EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    sentiment, confidence, probs = predict_sentiment(text, model, tokenizer, device)\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Text: {text}\")\n",
    "    print(f\"  Prediction: {sentiment}\")\n",
    "    print(f\"  Confidence: {confidence:.2f}%\")\n",
    "    print(f\"  Probabilities: [Negative: {probs[0]*100:.2f}%, Positive: {probs[1]*100:.2f}%]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14abd2d",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully:\n",
    "- ‚úÖ Set up TEN on Google Colab with T4 GPU\n",
    "- ‚úÖ Downloaded and prepared the IMDb dataset\n",
    "- ‚úÖ Trained a TEN model for sentiment classification\n",
    "- ‚úÖ Benchmarked the model's performance\n",
    "- ‚úÖ Visualized training results\n",
    "- ‚úÖ Tested inference on custom examples\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Try different datasets**: Replace IMDb with other datasets from Hugging Face\n",
    "2. **Experiment with hyperparameters**: Adjust `d_model`, `n_layers`, `num_eigenstates`\n",
    "3. **Longer sequences**: Test with sequences up to 2048 or 4096 tokens\n",
    "4. **Compare with Transformers**: Implement a standard Transformer baseline\n",
    "5. **Fine-tune**: Use the trained model as a starting point for related tasks\n",
    "\n",
    "---\n",
    "\n",
    "**Copyright (c) 2025 Genovo Technologies. All Rights Reserved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to save models\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create directory for models\n",
    "    save_dir = '/content/drive/MyDrive/TEN_models'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    print(f\"‚úì Google Drive mounted at {save_dir}\")\n",
    "    print(\"  Models will be automatically saved to Drive\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Could not mount Google Drive\")\n",
    "    print(\"  Models will only be saved locally (lost if runtime disconnects)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eab90d",
   "metadata": {},
   "source": [
    "### Issue 4: Runtime Disconnection\n",
    "\n",
    "**Solution:** Save checkpoints regularly and use Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94716b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU utilization\n",
    "if torch.cuda.is_available():\n",
    "    !nvidia-smi\n",
    "    \n",
    "    print(\"\\nTips for faster training:\")\n",
    "    print(\"  1. Increase BATCH_SIZE if GPU memory allows\")\n",
    "    print(\"  2. Use mixed precision training (add to future updates)\")\n",
    "    print(\"  3. Reduce num_workers in DataLoader if CPU is bottleneck\")\n",
    "    print(\"  4. Use smaller dataset subset for quick experiments\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected. Training will be slow on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7d7f9",
   "metadata": {},
   "source": [
    "### Issue 3: Slow Training\n",
    "\n",
    "**Solution:** Check GPU utilization and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed940852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if modules are importable\n",
    "import sys\n",
    "print(\"Python path:\")\n",
    "for p in sys.path[:5]:\n",
    "    print(f\"  {p}\")\n",
    "\n",
    "try:\n",
    "    from src.model import TemporalEigenstateConfig\n",
    "    print(\"\\n‚úì Model imports working\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚úó Import error: {e}\")\n",
    "    print(\"\\nTry running:\")\n",
    "    print(\"  !pip install -e /content/temporal-eigenstate-networks\")\n",
    "    print(\"  sys.path.insert(0, '/content/temporal-eigenstate-networks/src')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b9dd66",
   "metadata": {},
   "source": [
    "### Issue 2: Import Errors\n",
    "\n",
    "**Solution:** Verify installation and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you encounter OOM errors, try these settings:\n",
    "# BATCH_SIZE = 8  # Reduce from 16\n",
    "# MAX_SEQ_LENGTH = 256  # Reduce from 512\n",
    "# config.d_model = 128  # Reduce from 256\n",
    "# config.n_layers = 2  # Reduce from 4\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úì GPU cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5cc3dc",
   "metadata": {},
   "source": [
    "### Issue 1: Out of Memory\n",
    "\n",
    "**Solution:** Reduce batch size or sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227791b",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting\n",
    "\n",
    "Common issues and solutions:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
