"""
Optimized Training Script for DigitalOcean L40S/RTX 6000 Ada (48GB)

This script is optimized for:
- Long-range tasks (8192 token sequences)
- Large model training (1024 dim, 8+ layers)
- 48GB VRAM efficiency
- 5-hour training window

Usage:
    python train_digitalocean.py --config large
    python train_digitalocean.py --config medium --max_seq_len 4096
    python train_digitalocean.py --benchmark
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from datasets import load_dataset
from transformers import AutoTokenizer
import argparse
import time
import os
import sys
from pathlib import Path
from tqdm import tqdm
import json

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
from model import TemporalEigenstateConfig, TemporalEigenstateNetwork


# Predefined configurations optimized for 48GB GPU
CONFIGS = {
    "tiny": {
        "d_model": 256,
        "n_layers": 2,
        "num_eigenstates": 32,
        "batch_size": 64,
        "max_seq_len": 512,
        "description": "Quick test (~30 min, ~82% acc)",
    },
    "small": {
        "d_model": 512,
        "n_layers": 4,
        "num_eigenstates": 64,
        "batch_size": 32,
        "max_seq_len": 2048,
        "description": "Fast training (~1.5 hours, ~86% acc)",
    },
    "medium": {
        "d_model": 768,
        "n_layers": 6,
        "num_eigenstates": 96,
        "batch_size": 24,
        "max_seq_len": 4096,
        "description": "Balanced (~3 hours, ~88% acc)",
    },
    "large": {
        "d_model": 1024,
        "n_layers": 8,
        "num_eigenstates": 128,
        "batch_size": 16,
        "max_seq_len": 8192,
        "description": "Full power (~4.5 hours, ~90% acc)",
    },
    "xlarge": {
        "d_model": 1280,
        "n_layers": 12,
        "num_eigenstates": 160,
        "batch_size": 8,
        "max_seq_len": 8192,
        "description": "Maximum model (~6 hours, ~91% acc)",
    },
}


class DigitalOceanTrainer:
    """Optimized trainer for DigitalOcean L40S/RTX 6000 Ada GPUs"""
    
    def __init__(self, config, args):
        self.config = config
        self.args = args
        self.device = torch.device('cuda')
        self.start_time = time.time()
        self.hourly_rate = 1.57  # DigitalOcean rate
        
        # Setup output directories
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "checkpoints").mkdir(exist_ok=True)
        (self.output_dir / "logs").mkdir(exist_ok=True)
        
        print("=" * 80)
        print(f"DigitalOcean GPU Training - {args.config.upper()} Configuration")
        print("=" * 80)
        
    def print_cost(self):
        """Print current cost"""
        elapsed = time.time() - self.start_time
        hours = elapsed / 3600
        cost = hours * self.hourly_rate
        remaining = 15.00 - cost  # $15 free credit
        
        print(f"\nâ±ï¸  Elapsed: {elapsed/60:.1f} min | Cost: ${cost:.2f} | Remaining: ${remaining:.2f}")
        
    def create_model(self):
        """Create and configure model"""
        print(f"\nðŸ“¦ Creating model...")
        
        model_config = TemporalEigenstateConfig(
            d_model=self.config['d_model'],
            n_heads=self.config.get('n_heads', self.config['d_model'] // 64),
            n_layers=self.config['n_layers'],
            d_ff=self.config.get('d_ff', self.config['d_model'] * 4),
            max_seq_len=self.config['max_seq_len'],
            num_eigenstates=self.config['num_eigenstates'],
            dropout=self.config.get('dropout', 0.1),
            vocab_size=self.args.vocab_size,
        )
        
        model = TemporalEigenstateNetwork(model_config).to(self.device)
        
        # Count parameters
        num_params = sum(p.numel() for p in model.parameters())
        num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"  âœ“ Model created successfully")
        print(f"  Parameters: {num_params:,} ({num_params/1e6:.1f}M)")
        print(f"  Trainable: {num_trainable:,}")
        print(f"  Memory: ~{num_params * 4 / 1024**3:.2f} GB (fp32)")
        print(f"  Config: {model_config.d_model}d, {model_config.n_layers}L, {model_config.num_eigenstates}E")
        
        # Test forward pass
        print(f"\nðŸ§ª Testing forward pass...")
        test_input = torch.randint(0, self.args.vocab_size, (2, 128)).to(self.device)
        with torch.no_grad():
            test_output = model(test_input)
        print(f"  âœ“ Forward pass successful: {test_input.shape} -> {test_output.shape}")
        
        return model
    
    def prepare_data(self):
        """Load and prepare dataset"""
        print(f"\nðŸ“š Loading dataset: {self.args.dataset}...")

        # Map friendly dataset names to HuggingFace identifiers
        dataset_map = {
            "imdb": ("imdb", None),
            "ag_news": ("ag_news", None),
            "wikitext-103": ("wikitext", "wikitext-103-raw-v1"),
            "tinystories": ("roneneldan/TinyStories", None),
            "finewebedu": ("HuggingFaceFW/fineweb-edu", "sample-10BT"),
            "pg19": ("emozilla/pg19-test", None),
            "openwebtext": ("Skylion007/openwebtext", None),
        }

        if self.args.dataset not in dataset_map:
            raise ValueError(f"Unknown dataset: {self.args.dataset}")

        hf_name, hf_config = dataset_map[self.args.dataset]
        # Load dataset (some repos expose the dataset directly by repo name)
        if hf_config:
            dataset = load_dataset(hf_name, hf_config)
        else:
            dataset = load_dataset(hf_name)

        # Determine task type
        if self.args.dataset in ("imdb", "ag_news"):
            num_classes = 2 if self.args.dataset == "imdb" else 4
        else:
            num_classes = None  # language modeling / unsupervised

        print(f"  âœ“ Dataset loaded")
        # Some datasets don't have a test split
        train_len = len(dataset['train']) if 'train' in dataset else 0
        test_len = len(dataset['test']) if 'test' in dataset else 0
        print(f"  Train samples: {train_len:,}")
        print(f"  Test samples: {test_len:,}")

        # Use subset if specified
        if self.args.subset_size > 0 and train_len > 0:
            sel = min(self.args.subset_size, train_len)
            dataset['train'] = dataset['train'].shuffle(seed=42).select(range(sel))
            if test_len > 0:
                sel_test = min(self.args.subset_size // 5, test_len)
                dataset['test'] = dataset['test'].shuffle(seed=42).select(range(sel_test))
            print(f"  Using subset: {len(dataset['train'])} train, {len(dataset.get('test', []))} test")

        # Load tokenizer (use GPT-2 tokenizer by default for LM tasks)
        print(f"\nðŸ”¤ Loading tokenizer...")
        tokenizer_name = self.args.tokenizer or "gpt2"
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)
        # Ensure tokenizer has pad token
        if tokenizer.pad_token is None:
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})

        print(f"  âœ“ Tokenizer loaded (vocab: {len(tokenizer)})")

        return dataset, tokenizer, num_classes
    
    def benchmark(self):
        """Run comprehensive benchmarks"""
        print("\n" + "=" * 80)
        print("BENCHMARKING MODE")
        print("=" * 80)
        
        results = {}
        
        # Test different configurations
        test_configs = ["tiny", "small", "medium", "large"]
        sequence_lengths = [512, 1024, 2048, 4096, 8192]
        
        for config_name in test_configs:
            print(f"\nðŸ“Š Testing {config_name.upper()} configuration...")
            config = CONFIGS[config_name]
            
            # Create model
            model_config = TemporalEigenstateConfig(
                d_model=config['d_model'],
                n_layers=config['n_layers'],
                num_eigenstates=config['num_eigenstates'],
                max_seq_len=max(sequence_lengths),
                vocab_size=50000,
            )
            model = TemporalEigenstateNetwork(model_config).to(self.device)
            model.eval()
            
            config_results = {
                "parameters": sum(p.numel() for p in model.parameters()),
                "sequence_lengths": {}
            }
            
            # Test each sequence length
            for seq_len in sequence_lengths:
                if seq_len > config['max_seq_len']:
                    continue
                
                try:
                    batch_size = config['batch_size']
                    input_ids = torch.randint(0, 50000, (batch_size, seq_len)).to(self.device)
                    
                    # Warmup
                    with torch.no_grad():
                        for _ in range(3):
                            _ = model(input_ids)
                    
                    torch.cuda.synchronize()
                    torch.cuda.reset_peak_memory_stats()
                    
                    # Benchmark
                    times = []
                    with torch.no_grad():
                        for _ in range(10):
                            start = time.time()
                            _ = model(input_ids)
                            torch.cuda.synchronize()
                            times.append(time.time() - start)
                    
                    avg_time = sum(times) / len(times)
                    throughput = batch_size * seq_len / avg_time
                    memory = torch.cuda.max_memory_allocated() / 1024**3
                    
                    config_results["sequence_lengths"][seq_len] = {
                        "time_ms": avg_time * 1000,
                        "throughput": throughput,
                        "memory_gb": memory,
                        "batch_size": batch_size
                    }
                    
                    print(f"  {seq_len:5d} tokens: {avg_time*1000:6.2f}ms | {throughput:8.0f} tok/s | {memory:5.2f}GB")
                    
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print(f"  {seq_len:5d} tokens: OOM")
                        torch.cuda.empty_cache()
                    else:
                        raise
            
            results[config_name] = config_results
            
            del model
            torch.cuda.empty_cache()
        
        # Save results
        output_file = self.output_dir / "benchmark_results.json"
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nâœ“ Benchmark results saved to {output_file}")
        self.print_cost()
        
        return results
    
    def train(self):
        """Main training loop"""
        # Create model
        model = self.create_model()
        
        # Prepare data
        dataset, tokenizer, num_classes = self.prepare_data()

        # If dry run requested, run a quick forward pass and exit
        if self.args.dry_run:
            print("\nâš¡ Dry run: running a single forward pass with a small batch...")

            # Find a textual field in the dataset examples
            sample_example = dataset['train'][0]
            text_field = None
            for k, v in sample_example.items():
                if isinstance(v, str):
                    text_field = k
                    break

            if text_field is None:
                raise RuntimeError("No textual field found in dataset for dry run")

            n = max(1, min(self.args.dry_samples, len(dataset['train'])))
            texts = [dataset['train'][i][text_field] for i in range(n)]

            # Tokenize to model max length
            max_len = self.config.get('max_seq_len', 1024)
            tokenized = tokenizer(texts, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')
            input_ids = tokenized['input_ids'].to(self.device)

            model.eval()
            with torch.no_grad():
                out = model(input_ids)

            print(f"  âœ“ Dry run forward pass successful: input {input_ids.shape} -> output {out.shape}")
            print(f"  GPU memory (GB): {torch.cuda.max_memory_allocated() / 1024**3:.2f}")
            return
        
        # Setup training
        print(f"\nâš™ï¸  Setting up training...")
        
        # Optimizer
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=self.args.learning_rate,
            weight_decay=self.args.weight_decay,
            betas=(0.9, 0.999)
        )
        
        # Mixed precision
        use_amp = self.args.mixed_precision
        scaler = GradScaler() if use_amp else None
        print(f"  Mixed precision: {'Enabled' if use_amp else 'Disabled'}")
        
        # Scheduler
        total_steps = (len(dataset['train']) // self.config['batch_size']) * self.args.epochs
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_steps)
        
        print(f"  Optimizer: AdamW (lr={self.args.learning_rate})")
        print(f"  Batch size: {self.config['batch_size']}")
        print(f"  Gradient accumulation: {self.args.gradient_accumulation}")
        print(f"  Epochs: {self.args.epochs}")
        print(f"  Total steps: {total_steps:,}")
        
        # Training info
        print(f"\nðŸš€ Starting training...")
        print(f"  Expected time: {CONFIGS[self.args.config]['description']}")
        self.print_cost()
        
        # Prepare tokenized dataset for language modeling
        def tokenize_function(examples):
            # Find text field
            text_field = 'text'
            if text_field not in examples:
                # Try other common field names
                for field in ['content', 'article', 'document']:
                    if field in examples:
                        text_field = field
                        break
            
            return tokenizer(
                examples[text_field],
                truncation=True,
                max_length=self.config['max_seq_len'],
                padding='max_length',
                return_tensors='pt'
            )
        
        print("\nðŸ“ Tokenizing dataset...")
        tokenized_train = dataset['train'].map(
            tokenize_function,
            batched=True,
            remove_columns=dataset['train'].column_names,
            desc="Tokenizing train"
        )
        tokenized_train.set_format('torch')
        
        # Create data loader
        train_loader = DataLoader(
            tokenized_train,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=2,
            pin_memory=True
        )
        
        print(f"  âœ“ Data loader ready ({len(train_loader)} batches per epoch)")
        
        # Training loop
        model.train()
        global_step = 0
        best_loss = float('inf')
        
        for epoch in range(self.args.epochs):
            print(f"\n{'='*80}")
            print(f"Epoch {epoch + 1}/{self.args.epochs}")
            print(f"{'='*80}")
            
            epoch_loss = 0
            optimizer.zero_grad()
            
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
            
            for batch_idx, batch in enumerate(progress_bar):
                input_ids = batch['input_ids'].to(self.device)
                
                # Create labels for language modeling (predict next token)
                # Shift by one: input=[0,1,2,3], label=[1,2,3,PAD]
                labels = input_ids[:, 1:].contiguous()
                inputs = input_ids[:, :-1].contiguous()
                
                # Forward pass
                if use_amp:
                    with autocast('cuda'):
                        outputs = model(inputs)
                        # outputs shape: (batch, seq_len-1, vocab_size)
                        loss = nn.functional.cross_entropy(
                            outputs.reshape(-1, outputs.size(-1)),
                            labels.reshape(-1),
                            ignore_index=tokenizer.pad_token_id if tokenizer.pad_token_id else -100
                        )
                        loss = loss / self.args.gradient_accumulation
                    
                    scaler.scale(loss).backward()
                else:
                    outputs = model(inputs)
                    loss = nn.functional.cross_entropy(
                        outputs.reshape(-1, outputs.size(-1)),
                        labels.reshape(-1),
                        ignore_index=tokenizer.pad_token_id if tokenizer.pad_token_id else -100
                    )
                    loss = loss / self.args.gradient_accumulation
                    loss.backward()
                
                epoch_loss += loss.item() * self.args.gradient_accumulation
                
                # Gradient accumulation
                if (batch_idx + 1) % self.args.gradient_accumulation == 0:
                    if use_amp:
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        optimizer.step()
                    
                    scheduler.step()
                    optimizer.zero_grad()
                    global_step += 1
                
                # Update progress bar
                progress_bar.set_postfix({
                    'loss': f'{loss.item() * self.args.gradient_accumulation:.4f}',
                    'lr': f'{scheduler.get_last_lr()[0]:.2e}'
                })
                
                # Save checkpoint
                if global_step > 0 and global_step % self.args.save_steps == 0:
                    checkpoint_path = self.output_dir / "checkpoints" / f"checkpoint-{global_step}.pt"
                    torch.save({
                        'step': global_step,
                        'epoch': epoch,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'loss': loss.item(),
                    }, checkpoint_path)
                    print(f"\nðŸ’¾ Checkpoint saved: {checkpoint_path}")
            
            # Epoch summary
            avg_loss = epoch_loss / len(train_loader)
            print(f"\nðŸ“Š Epoch {epoch + 1} Summary:")
            print(f"  Average loss: {avg_loss:.4f}")
            self.print_cost()
            
            # Save best model
            if avg_loss < best_loss:
                best_loss = avg_loss
                best_path = self.output_dir / "checkpoints" / "best_model.pt"
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'loss': best_loss,
                    'config': self.config,
                }, best_path)
                print(f"  â­ New best model saved: {best_path}")
        
        # Final save
        final_path = self.output_dir / "checkpoints" / "final_model.pt"
        torch.save({
            'model_state_dict': model.state_dict(),
            'config': self.config,
            'total_steps': global_step,
        }, final_path)
        
        print(f"\n{'='*80}")
        print(f"âœ… Training Complete!")
        print(f"{'='*80}")
        print(f"  Total steps: {global_step}")
        print(f"  Final loss: {avg_loss:.4f}")
        print(f"  Best loss: {best_loss:.4f}")
        print(f"  Model saved: {final_path}")
        self.print_cost()


def main():
    parser = argparse.ArgumentParser(description="Train TEN on DigitalOcean GPU")
    
    # Model configuration
    parser.add_argument("--config", type=str, default="medium",
                       choices=list(CONFIGS.keys()),
                       help="Model configuration preset")
    
    # Training parameters
    parser.add_argument("--dataset", type=str, default="imdb",
                       choices=["imdb", "ag_news", "wikitext-103", "tinystories", "finewebedu", "pg19", "openwebtext"],
                       help="Dataset to use")
    parser.add_argument("--epochs", type=int, default=3,
                       help="Number of epochs")
    parser.add_argument("--learning_rate", type=float, default=3e-4,
                       help="Learning rate")
    parser.add_argument("--weight_decay", type=float, default=0.01,
                       help="Weight decay")
    parser.add_argument("--gradient_accumulation", type=int, default=1,
                       help="Gradient accumulation steps")
    parser.add_argument("--mixed_precision", action="store_true",
                       help="Use mixed precision training (faster)")
    
    # Data parameters
    parser.add_argument("--vocab_size", type=int, default=50257,
                       help="Vocabulary size (50257 for GPT-2, 30522 for BERT)")
    parser.add_argument("--subset_size", type=int, default=0,
                       help="Use dataset subset (0 = full dataset)")
    parser.add_argument("--max_seq_len", type=int, default=None,
                       help="Override max sequence length")
    parser.add_argument("--save_steps", type=int, default=1000,
                       help="Save checkpoint every N steps")

    # Dry-run and tokenizer options
    parser.add_argument("--dry_run", action="store_true",
                       help="Quickly build model and run one forward pass using a small batch")
    parser.add_argument("--dry_samples", type=int, default=2,
                       help="Number of samples to use for --dry_run")
    parser.add_argument("--tokenizer", type=str, default=None,
                       help="Tokenizer name to use (e.g. gpt2, bert-base-uncased). Default: gpt2")
    
    # Output
    parser.add_argument("--output_dir", type=str, 
                       default=os.path.expanduser("~/ten_workspace"),
                       help="Output directory")
    
    # Special modes
    parser.add_argument("--benchmark", action="store_true",
                       help="Run benchmark mode instead of training")
    
    args = parser.parse_args()
    
    # Get configuration
    config = CONFIGS[args.config].copy()
    if args.max_seq_len:
        config['max_seq_len'] = args.max_seq_len
    
    # Print configuration
    print("\n" + "=" * 80)
    print(f"DigitalOcean L40S/RTX 6000 Ada Training")
    print("=" * 80)
    print(f"\nConfiguration: {args.config.upper()}")
    print(f"Description: {config['description']}")
    print(f"Parameters:")
    for key, value in config.items():
        if key != 'description':
            print(f"  {key}: {value}")
    print("\nGPU: 48GB VRAM (L40S or RTX 6000 Ada)")
    print("Cost: $1.57/hour")
    print("Budget: $15 free credit (~9.5 hours)")
    print("=" * 80)
    
    # Create trainer
    trainer = DigitalOceanTrainer(config, args)
    
    # Run benchmark or training
    if args.benchmark:
        trainer.benchmark()
    else:
        trainer.train()


if __name__ == "__main__":
    main()
