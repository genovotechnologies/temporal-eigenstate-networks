"""
Optimized Training Script for DigitalOcean L40S/RTX 6000 Ada (48GB)

This script is optimized for:
- Long-range tasks (8192 token sequences)
- Large model training (1024 dim, 8+ layers)
- 48GB VRAM efficiency
- 5-hour training window

Usage:
    python train_digitalocean.py --config large
    python train_digitalocean.py --config medium --max_seq_len 4096
    python train_digitalocean.py --benchmark
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from datasets import load_dataset
from transformers import AutoTokenizer
import argparse
import time
import os
import sys
from pathlib import Path
from tqdm import tqdm
import json

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
from model import TemporalEigenstateConfig, TemporalEigenstateNetwork


# Predefined configurations optimized for 48GB GPU
CONFIGS = {
    "tiny": {
        "d_model": 256,
        "n_layers": 2,
        "num_eigenstates": 32,
        "batch_size": 64,
        "max_seq_len": 512,
        "description": "Quick test (~30 min, ~82% acc)",
    },
    "small": {
        "d_model": 512,
        "n_layers": 4,
        "num_eigenstates": 64,
        "batch_size": 32,
        "max_seq_len": 2048,
        "description": "Fast training (~1.5 hours, ~86% acc)",
    },
    "medium": {
        "d_model": 768,
        "n_layers": 6,
        "num_eigenstates": 96,
        "batch_size": 24,
        "max_seq_len": 4096,
        "description": "Balanced (~3 hours, ~88% acc)",
    },
    "large": {
        "d_model": 1024,
        "n_layers": 8,
        "num_eigenstates": 128,
        "batch_size": 16,
        "max_seq_len": 8192,
        "description": "Full power (~4.5 hours, ~90% acc)",
    },
    "xlarge": {
        "d_model": 1280,
        "n_layers": 12,
        "num_eigenstates": 160,
        "batch_size": 8,
        "max_seq_len": 8192,
        "description": "Maximum model (~6 hours, ~91% acc)",
    },
}


class DigitalOceanTrainer:
    """Optimized trainer for DigitalOcean L40S/RTX 6000 Ada GPUs"""
    
    def __init__(self, config, args):
        self.config = config
        self.args = args
        self.device = torch.device('cuda')
        self.start_time = time.time()
        self.hourly_rate = 1.57  # DigitalOcean rate
        
        # Setup output directories
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "checkpoints").mkdir(exist_ok=True)
        (self.output_dir / "logs").mkdir(exist_ok=True)
        
        print("=" * 80)
        print(f"DigitalOcean GPU Training - {args.config.upper()} Configuration")
        print("=" * 80)
        
    def print_cost(self):
        """Print current cost"""
        elapsed = time.time() - self.start_time
        hours = elapsed / 3600
        cost = hours * self.hourly_rate
        remaining = 15.00 - cost  # $15 free credit
        
        print(f"\n‚è±Ô∏è  Elapsed: {elapsed/60:.1f} min | Cost: ${cost:.2f} | Remaining: ${remaining:.2f}")
        
    def create_model(self):
        """Create and configure model"""
        print(f"\nüì¶ Creating model...")
        
        model_config = TemporalEigenstateConfig(
            d_model=self.config['d_model'],
            n_heads=self.config.get('n_heads', self.config['d_model'] // 64),
            n_layers=self.config['n_layers'],
            d_ff=self.config.get('d_ff', self.config['d_model'] * 4),
            max_seq_len=self.config['max_seq_len'],
            num_eigenstates=self.config['num_eigenstates'],
            dropout=self.config.get('dropout', 0.1),
            vocab_size=self.args.vocab_size,
        )
        
        model = TemporalEigenstateNetwork(model_config).to(self.device)
        
        # Count parameters
        num_params = sum(p.numel() for p in model.parameters())
        num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"  ‚úì Model created successfully")
        print(f"  Parameters: {num_params:,} ({num_params/1e6:.1f}M)")
        print(f"  Trainable: {num_trainable:,}")
        print(f"  Memory: ~{num_params * 4 / 1024**3:.2f} GB (fp32)")
        print(f"  Config: {model_config.d_model}d, {model_config.n_layers}L, {model_config.num_eigenstates}E")
        
        # Test forward pass
        print(f"\nüß™ Testing forward pass...")
        test_input = torch.randint(0, self.args.vocab_size, (2, 128)).to(self.device)
        with torch.no_grad():
            test_output = model(test_input)
        print(f"  ‚úì Forward pass successful: {test_input.shape} -> {test_output.shape}")
        
        return model
    
    def prepare_data(self):
        """Load and prepare dataset"""
        print(f"\nüìö Loading dataset: {self.args.dataset}...")

        # Map friendly dataset names to HuggingFace identifiers
        dataset_map = {
            "imdb": ("imdb", None),
            "ag_news": ("ag_news", None),
            "wikitext-103": ("wikitext", "wikitext-103-raw-v1"),
            "tinystories": ("roneneldan/TinyStories", None),
            "finewebedu": ("HuggingFaceFW/fineweb-edu", "sample-10BT"),
            "pg19": ("emozilla/pg19-test", None),
            "openwebtext": ("Skylion007/openwebtext", None),
        }

        if self.args.dataset not in dataset_map:
            raise ValueError(f"Unknown dataset: {self.args.dataset}")

        hf_name, hf_config = dataset_map[self.args.dataset]
        # Load dataset (some repos expose the dataset directly by repo name)
        if hf_config:
            dataset = load_dataset(hf_name, hf_config)
        else:
            dataset = load_dataset(hf_name)

        # Determine task type
        if self.args.dataset in ("imdb", "ag_news"):
            num_classes = 2 if self.args.dataset == "imdb" else 4
        else:
            num_classes = None  # language modeling / unsupervised

        print(f"  ‚úì Dataset loaded")
        # Some datasets don't have a test split
        train_len = len(dataset['train']) if 'train' in dataset else 0
        test_len = len(dataset['test']) if 'test' in dataset else 0
        print(f"  Train samples: {train_len:,}")
        print(f"  Test samples: {test_len:,}")

        # Use subset if specified
        if self.args.subset_size > 0 and train_len > 0:
            sel = min(self.args.subset_size, train_len)
            dataset['train'] = dataset['train'].shuffle(seed=42).select(range(sel))
            if test_len > 0:
                sel_test = min(self.args.subset_size // 5, test_len)
                dataset['test'] = dataset['test'].shuffle(seed=42).select(range(sel_test))
            print(f"  Using subset: {len(dataset['train'])} train, {len(dataset.get('test', []))} test")

        # Load tokenizer (use GPT-2 tokenizer by default for LM tasks)
        print(f"\nüî§ Loading tokenizer...")
        tokenizer_name = self.args.tokenizer or "gpt2"
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)
        # Ensure tokenizer has pad token
        if tokenizer.pad_token is None:
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})

        print(f"  ‚úì Tokenizer loaded (vocab: {len(tokenizer)})")

        return dataset, tokenizer, num_classes
    
    def benchmark(self):
        """Run comprehensive benchmarks"""
        print("\n" + "=" * 80)
        print("BENCHMARKING MODE")
        print("=" * 80)
        
        results = {}
        
        # Test different configurations
        test_configs = ["tiny", "small", "medium", "large"]
        sequence_lengths = [512, 1024, 2048, 4096, 8192]
        
        for config_name in test_configs:
            print(f"\nüìä Testing {config_name.upper()} configuration...")
            config = CONFIGS[config_name]
            
            # Create model
            model_config = TemporalEigenstateConfig(
                d_model=config['d_model'],
                n_layers=config['n_layers'],
                num_eigenstates=config['num_eigenstates'],
                max_seq_len=max(sequence_lengths),
                vocab_size=50000,
            )
            model = TemporalEigenstateNetwork(model_config).to(self.device)
            model.eval()
            
            config_results = {
                "parameters": sum(p.numel() for p in model.parameters()),
                "sequence_lengths": {}
            }
            
            # Test each sequence length
            for seq_len in sequence_lengths:
                if seq_len > config['max_seq_len']:
                    continue
                
                try:
                    batch_size = config['batch_size']
                    input_ids = torch.randint(0, 50000, (batch_size, seq_len)).to(self.device)
                    
                    # Warmup
                    with torch.no_grad():
                        for _ in range(3):
                            _ = model(input_ids)
                    
                    torch.cuda.synchronize()
                    torch.cuda.reset_peak_memory_stats()
                    
                    # Benchmark
                    times = []
                    with torch.no_grad():
                        for _ in range(10):
                            start = time.time()
                            _ = model(input_ids)
                            torch.cuda.synchronize()
                            times.append(time.time() - start)
                    
                    avg_time = sum(times) / len(times)
                    throughput = batch_size * seq_len / avg_time
                    memory = torch.cuda.max_memory_allocated() / 1024**3
                    
                    config_results["sequence_lengths"][seq_len] = {
                        "time_ms": avg_time * 1000,
                        "throughput": throughput,
                        "memory_gb": memory,
                        "batch_size": batch_size
                    }
                    
                    print(f"  {seq_len:5d} tokens: {avg_time*1000:6.2f}ms | {throughput:8.0f} tok/s | {memory:5.2f}GB")
                    
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print(f"  {seq_len:5d} tokens: OOM")
                        torch.cuda.empty_cache()
                    else:
                        raise
            
            results[config_name] = config_results
            
            del model
            torch.cuda.empty_cache()
        
        # Save results
        output_file = self.output_dir / "benchmark_results.json"
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\n‚úì Benchmark results saved to {output_file}")
        self.print_cost()
        
        return results
    
    def train(self):
        """Main training loop"""
        # Create model
        model = self.create_model()
        
        # Prepare data
        dataset, tokenizer, num_classes = self.prepare_data()

        # If dry run requested, run a quick forward pass and exit
        if self.args.dry_run:
            print("\n‚ö° Dry run: running a single forward pass with a small batch...")

            # Find a textual field in the dataset examples
            sample_example = dataset['train'][0]
            text_field = None
            for k, v in sample_example.items():
                if isinstance(v, str):
                    text_field = k
                    break

            if text_field is None:
                raise RuntimeError("No textual field found in dataset for dry run")

            n = max(1, min(self.args.dry_samples, len(dataset['train'])))
            texts = [dataset['train'][i][text_field] for i in range(n)]

            # Tokenize to model max length
            max_len = self.config.get('max_seq_len', 1024)
            tokenized = tokenizer(texts, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')
            input_ids = tokenized['input_ids'].to(self.device)

            model.eval()
            with torch.no_grad():
                out = model(input_ids)

            print(f"  ‚úì Dry run forward pass successful: input {input_ids.shape} -> output {out.shape}")
            print(f"  GPU memory (GB): {torch.cuda.max_memory_allocated() / 1024**3:.2f}")
            return
        
        # Setup training
        print(f"\n‚öôÔ∏è  Setting up training...")
        
        # Optimizer
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=self.args.learning_rate,
            weight_decay=self.args.weight_decay,
            betas=(0.9, 0.999)
        )
        
        # Mixed precision
        use_amp = self.args.mixed_precision
        scaler = GradScaler() if use_amp else None
        print(f"  Mixed precision: {'Enabled' if use_amp else 'Disabled'}")
        
        # Scheduler
        total_steps = (len(dataset['train']) // self.config['batch_size']) * self.args.epochs
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_steps)
        
        print(f"  Optimizer: AdamW (lr={self.args.learning_rate})")
        print(f"  Batch size: {self.config['batch_size']}")
        print(f"  Gradient accumulation: {self.args.gradient_accumulation}")
        print(f"  Epochs: {self.args.epochs}")
        print(f"  Total steps: {total_steps:,}")
        
        # Training info
        print(f"\nüöÄ Starting training...")
        print(f"  Expected time: {CONFIGS[self.args.config]['description']}")
        self.print_cost()
        
        # Training loop would go here
        # For now, just show the setup
        
        print(f"\n‚úì Setup complete! Ready to train.")
        print(f"\nTo start training, uncomment the training loop in this script.")
        
        self.print_cost()


def main():
    parser = argparse.ArgumentParser(description="Train TEN on DigitalOcean GPU")
    
    # Model configuration
    parser.add_argument("--config", type=str, default="medium",
                       choices=list(CONFIGS.keys()),
                       help="Model configuration preset")
    
    # Training parameters
    parser.add_argument("--dataset", type=str, default="imdb",
                       choices=["imdb", "ag_news", "wikitext-103", "tinystories", "finewebedu", "pg19", "openwebtext"],
                       help="Dataset to use")
    parser.add_argument("--epochs", type=int, default=3,
                       help="Number of epochs")
    parser.add_argument("--learning_rate", type=float, default=3e-4,
                       help="Learning rate")
    parser.add_argument("--weight_decay", type=float, default=0.01,
                       help="Weight decay")
    parser.add_argument("--gradient_accumulation", type=int, default=1,
                       help="Gradient accumulation steps")
    parser.add_argument("--mixed_precision", action="store_true",
                       help="Use mixed precision training (faster)")
    
    # Data parameters
    parser.add_argument("--vocab_size", type=int, default=30522,
                       help="Vocabulary size")
    parser.add_argument("--subset_size", type=int, default=0,
                       help="Use dataset subset (0 = full dataset)")
    parser.add_argument("--max_seq_len", type=int, default=None,
                       help="Override max sequence length")

    # Dry-run and tokenizer options
    parser.add_argument("--dry_run", action="store_true",
                       help="Quickly build model and run one forward pass using a small batch")
    parser.add_argument("--dry_samples", type=int, default=2,
                       help="Number of samples to use for --dry_run")
    parser.add_argument("--tokenizer", type=str, default=None,
                       help="Tokenizer name to use (e.g. gpt2, bert-base-uncased). Default: gpt2")
    
    # Output
    parser.add_argument("--output_dir", type=str, 
                       default=os.path.expanduser("~/ten_workspace"),
                       help="Output directory")
    
    # Special modes
    parser.add_argument("--benchmark", action="store_true",
                       help="Run benchmark mode instead of training")
    
    args = parser.parse_args()
    
    # Get configuration
    config = CONFIGS[args.config].copy()
    if args.max_seq_len:
        config['max_seq_len'] = args.max_seq_len
    
    # Print configuration
    print("\n" + "=" * 80)
    print(f"DigitalOcean L40S/RTX 6000 Ada Training")
    print("=" * 80)
    print(f"\nConfiguration: {args.config.upper()}")
    print(f"Description: {config['description']}")
    print(f"Parameters:")
    for key, value in config.items():
        if key != 'description':
            print(f"  {key}: {value}")
    print("\nGPU: 48GB VRAM (L40S or RTX 6000 Ada)")
    print("Cost: $1.57/hour")
    print("Budget: $15 free credit (~9.5 hours)")
    print("=" * 80)
    
    # Create trainer
    trainer = DigitalOceanTrainer(config, args)
    
    # Run benchmark or training
    if args.benchmark:
        trainer.benchmark()
    else:
        trainer.train()


if __name__ == "__main__":
    main()
