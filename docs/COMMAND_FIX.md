# ðŸš¨ IMPORTANT: Command Fixes

## Issues with Your Command

### âŒ Issue 1: Conflicting Flags
```bash
--pretokenized \
--streaming \
```

**Problem:** You can't use BOTH!
- `--pretokenized` = Load pre-tokenized chunks from disk
- `--streaming` = Stream raw data from HuggingFace

**These are mutually exclusive modes!**

---

### âŒ Issue 2: RAM Concerns (FIXED!)
You said:
> "pretokenized data is around 76gb in size and my ram is 64"

**GOOD NEWS:** I just fixed the code!
- âœ… Old code: Loaded all 76GB to RAM (would crash!)
- âœ… New code: Loads chunks on-demand from disk (no RAM explosion!)

**With DataLoader workers (6 workers), each loads chunks as needed:**
- Worker 1: Loads chunk â†’ trains â†’ releases â†’ loads next chunk
- Worker 2: Loads chunk â†’ trains â†’ releases â†’ loads next chunk
- etc.

**Peak RAM usage: ~6 chunks Ã— 32K tokens Ã— 2 bytes = ~400MB (not 76GB!)**

---

## âœ… CORRECTED COMMAND

### Option 1: Pre-tokenized (FASTEST - Use This!)

```bash
# Create logs dir
mkdir -p ~/ten_workspace/logs

# Stop previous session
tmux kill-session -t training 2>/dev/null || true

# Launch training
tmux new -s training -d 'python3 examples/train_digitalocean.py \
  --config large \
  --dataset finewebedu \
  --pretokenized \
  --tokenized_dir /root/ten_workspace/tokenized/finewebedu \
  --epochs 1 \
  --mixed_precision \
  --gradient_accumulation 4 \
  --save_steps 2500 \
  --num_workers 6 \
  --learning_rate 3e-4 \
  --output_dir /root/ten_workspace 2>&1 | tee /root/ten_workspace/logs/training.log'
```

**Key changes:**
- âŒ Removed `--streaming` (conflicts with `--pretokenized`)
- âœ… Code now loads chunks on-demand (no RAM issues!)
- âœ… 6 workers will efficiently pipeline chunk loading

---

### Option 2: Streaming (If You Don't Have Pre-tokenized Data)

```bash
# If you DON'T have pre-tokenized chunks yet
tmux new -s training -d 'python3 examples/train_digitalocean.py \
  --config large \
  --dataset finewebedu \
  --streaming \
  --epochs 1 \
  --mixed_precision \
  --gradient_accumulation 4 \
  --save_steps 2500 \
  --learning_rate 3e-4 \
  --output_dir /root/ten_workspace 2>&1 | tee /root/ten_workspace/logs/training.log'
```

**Notes:**
- âŒ Removed `--pretokenized` and `--tokenized_dir`
- âŒ Removed `--num_workers` (streaming is single-threaded)
- âœ… Will stream from HuggingFace and tokenize on-the-fly

---

## ðŸŽ¯ Which Should You Use?

### Use PRE-TOKENIZED if:
- âœ… You already ran `pretokenize_and_pack.py`
- âœ… Chunks exist in `/root/ten_workspace/tokenized/finewebedu/`
- âœ… You want 5-50Ã— faster training
- âœ… **RAM is NOT an issue anymore** (fixed to load on-demand!)

### Use STREAMING if:
- âœ… You DON'T have pre-tokenized data yet
- âœ… You want to start training immediately (0 wait)
- âœ… You're okay with slower training

---

## ðŸ“Š Performance with Your Setup

### Your Hardware:
- RAM: 64GB
- GPU: 48GB L40S
- Disk: Has 76GB pre-tokenized chunks

### With Pre-tokenized (RECOMMENDED):
```
âœ… Chunks loaded on-demand from disk
âœ… Peak RAM usage: ~400MB (6 workers Ã— ~70MB per chunk)
âœ… GPU fully utilized: 38-42GB
âœ… Training speed: 15,000-25,000 tokens/sec
âœ… Time per epoch: ~1.5-2 hours
```

### With Streaming:
```
âš ï¸ Downloads from HuggingFace on-the-fly
âš ï¸ Tokenizes during training
âš ï¸ GPU partially idle: 20-30GB
âš ï¸ Training speed: 3,000-5,000 tokens/sec
âš ï¸ Time per epoch: ~5-7 hours
```

---

## âœ… FINAL RECOMMENDATION

**Use the pre-tokenized command (Option 1)!**

```bash
mkdir -p ~/ten_workspace/logs
tmux kill-session -t training 2>/dev/null || true

tmux new -s training -d 'python3 examples/train_digitalocean.py \
  --config large \
  --dataset finewebedu \
  --pretokenized \
  --tokenized_dir /root/ten_workspace/tokenized/finewebedu \
  --epochs 1 \
  --mixed_precision \
  --gradient_accumulation 4 \
  --save_steps 2500 \
  --num_workers 6 \
  --learning_rate 3e-4 \
  --output_dir /root/ten_workspace 2>&1 | tee /root/ten_workspace/logs/training.log'

# Monitor
tmux attach -t training
```

**Why this works with 76GB chunks and 64GB RAM:**
- âœ… I fixed the code to load chunks on-demand (not all at once!)
- âœ… 6 workers Ã— ~70MB = ~420MB peak RAM usage
- âœ… Rest of your 64GB RAM is for model, optimizer, gradients
- âœ… Perfect balance!

---

## ðŸ” Monitoring Commands

```bash
# Attach to training session
tmux attach -t training

# Watch logs in separate terminal
tail -f ~/ten_workspace/logs/training.log

# Monitor GPU usage
watch -n 1 nvidia-smi

# Check RAM usage
htop

# Stop training
tmux kill-session -t training
```

---

## ðŸ› If You See "Out of Memory"

### If GPU OOM (unlikely):
```bash
# Increase gradient accumulation
--gradient_accumulation 8  # Instead of 4
```

### If RAM OOM (very unlikely now):
```bash
# Reduce DataLoader workers
--num_workers 4  # Instead of 6
```

---

## ðŸŽ‰ Summary

**Your original concern:**
> "pretokenized data is around 76gb and my ram is 64"

**Solution:**
âœ… I fixed the code! Chunks are now loaded on-demand (not all at once)
âœ… Peak RAM usage: ~400MB (not 76GB!)
âœ… You can safely use pre-tokenized mode
âœ… Remove `--streaming` flag (conflicts with `--pretokenized`)

**Run this command NOW:**
```bash
bash scripts/start_training.sh
```

Or use the corrected command from Option 1 above! ðŸš€
