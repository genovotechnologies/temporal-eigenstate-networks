#!/bin/bash
# Restart training with optimized batch sizes that actually use your 48GB GPU!

echo "üõë Killing current training (underutilizing GPU)..."
pkill -f train_digitalocean.py

echo "‚è≥ Waiting for process to stop..."
sleep 3

echo "üßπ Clearing GPU memory..."
nvidia-smi --gpu-reset || true

echo ""
echo "üìä GPU Status Before:"
nvidia-smi

echo ""
echo "="*80
echo "üöÄ RESTARTING TRAINING WITH MASSIVE MODEL"
echo "="*80
echo ""
echo "üî• OLD CONFIG (pathetic):"
echo "  Model: 164M parameters"
echo "  Batch: 16"
echo "  Context: 8K tokens"
echo "  GPU usage: 1.2GB / 48GB (2.5%)"
echo ""
echo "üí™ NEW CONFIG (BEAST MODE):"
echo "  Model: 1.8 BILLION parameters"
echo "  Batch: 16 (with 32K context!)"
echo "  Context: 32,768 tokens (4√ó longer!)"
echo "  GPU usage: ~35-40GB / 48GB (80-85%)"
echo ""
echo "Expected improvements:"
echo "  - 11√ó more parameters (164M ‚Üí 1.8B)"
echo "  - 4√ó longer context (8K ‚Üí 32K)"
echo "  - 30√ó more GPU memory used"
echo "  - REAL billion-parameter model!"
echo ""

cd /root/temporal-eigenstate-networks
source /root/ten_venv/bin/activate

# Get the updated script from repo
git pull origin main || echo "‚ö†Ô∏è  Could not pull latest - using local version"

echo "Starting BEAST MODE training in 3 seconds..."
sleep 3

# Run with MASSIVE config
python3 examples/train_digitalocean.py \
    --config large \
    --dataset finewebedu \
    --epochs 2 \
    --mixed_precision \
    --save_steps 2500 \
    --max_seq_len 32768 \
    --gradient_accumulation 2

echo ""
echo "‚úÖ Training restarted with optimized configuration!"
