#!/bin/bash
# Quick restart script with OPTIMIZED data loading

echo "ðŸš€ Restarting training with AGGRESSIVE PREFETCHING..."
echo ""
echo "âš¡ Optimizations enabled:"
echo "  â€¢ LRU cache: 256 chunks in RAM (~4GB cache)"
echo "  â€¢ Prefetch: 16 batches per worker (6 workers = 96 batches ahead!)"
echo "  â€¢ Persistent workers: Keep processes alive between epochs"
echo "  â€¢ Fork context: Faster worker startup"
echo ""

# Stop any existing session
tmux kill-session -t training 2>/dev/null || true

# Create logs directory
mkdir -p ~/ten_workspace/logs

# Launch training with OPTIMIZED data loading
tmux new -s training -d "cd /root/temporal-eigenstate-networks && python3 examples/train_digitalocean.py \
  --config large \
  --dataset finewebedu \
  --pretokenized \
  --tokenized_dir /root/ten_workspace/tokenized/finewebedu \
  --epochs 1 \
  --mixed_precision \
  --gradient_accumulation 4 \
  --save_steps 2500 \
  --num_workers 6 \
  --learning_rate 3e-4 \
  --output_dir /root/ten_workspace 2>&1 | tee /root/ten_workspace/logs/training.log"

echo ""
echo "âœ… Training restarted with OPTIMIZED data loading!"
echo ""
echo "ðŸ“Š Data loading improvements:"
echo "  â€¢ Cache: 256 chunks Ã— 16MB = 4GB in RAM (hot data)"
echo "  â€¢ Pipeline: 6 workers Ã— 16 prefetch = 96 batches ready!"
echo "  â€¢ Total: ~1.5GB data always ready (GPU never waits!)"
echo ""
echo "ï¿½ Monitor commands:"
echo "  tmux attach -t training                          # Attach to session"
echo "  tail -f ~/ten_workspace/logs/training.log        # Watch logs"
echo "  watch -n 1 nvidia-smi                            # GPU usage"
echo ""
echo "ðŸ”¥ Training 1.8B parameter model on 10B tokens!"
echo "   Expected: ~1.5 hours (faster now!), ~$2.50 cost"
